% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%\documentclass[runningheads]{llncs}
%\documentclass[10pt,letterpaper,twocolumn]{article}
%\documentclass{sig-alternate}
\documentclass[10pt, conference, compsocconf]{IEEEtran}


% packages
\usepackage{xspace}
\usepackage{ifthen}
\usepackage{amsbsy}
\usepackage{amssymb}
\usepackage{balance}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{needspace}
\usepackage{microtype}
\usepackage{bold-extra}
\usepackage{subfigure}
\usepackage{wrapfig}


% constants
%\newcommand{\Title}{Tracking Performance Evolution}
\newcommand{\Title}{Multidimensional Profiling}
\newcommand{\TitleShort}{\Title}
\newcommand{\Authors}{Juan Pablo Sandoval Alcocer}
\newcommand{\AuthorsShort}{J. P. Sandoval}

% references
\usepackage[colorlinks]{hyperref}
\usepackage[all]{hypcap}
\setcounter{tocdepth}{2}
\hypersetup{
	colorlinks=true,
	urlcolor=black,
	linkcolor=black,
	citecolor=black,
	plainpages=false,
	bookmarksopen=true,
	pdfauthor={\Authors},
	pdftitle={\Title}}

\def\chapterautorefname{Chapter}
\def\appendixautorefname{Appendix}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\figureautorefname{Figure}
\def\tableautorefname{Table}
\def\listingautorefname{Listing}

% source code
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{listings}
\definecolor{source}{gray}{0.9}
\lstset{
	language={},
	% characters
	tabsize=3,
	upquote=true,
	escapechar={!},
	keepspaces=true,
	breaklines=true,
	alsoletter={\#:},
	breakautoindent=true,
	columns=fullflexible,
	showstringspaces=false,
	basicstyle=\footnotesize\sffamily,
	% background
	frame=single,
    framerule=0pt,
	backgroundcolor=\color{source},
	% numbering
	numbersep=5pt,
	numberstyle=\tiny,
	numberfirstline=true,
	% captioning
	captionpos=b,
	% formatting (html)
	moredelim=[is][\textbf]{<b>}{</b>},
	moredelim=[is][\textit]{<i>}{</i>},
	moredelim=[is][\color{red}\uwave]{<u>}{</u>},
	moredelim=[is][\color{red}\sout]{<del>}{</del>},
	moredelim=[is][\color{blue}\underline]{<ins>}{</ins>}}
\newcommand{\ct}{\lstinline[backgroundcolor=\color{white},basicstyle=\footnotesize\ttfamily]}
\newcommand{\lct}[1]{{\small\tt #1}}

% tikz
% \usepackage{tikz}
% \usetikzlibrary{matrix}
% \usetikzlibrary{arrows}
% \usetikzlibrary{external}
% \usetikzlibrary{positioning}
% \usetikzlibrary{shapes.multipart}
% 
% \tikzset{
% 	every 	picture/.style={semithick},
% 	every text node part/.style={align=center}}

% proof-reading
\usepackage{xcolor}
\usepackage[normalem]{ulem}
\newcommand{\ra}{$\rightarrow$}
\newcommand{\ugh}[1]{\textcolor{red}{\uwave{#1}}} % please rephrase
\newcommand{\ins}[1]{\textcolor{blue}{\uline{#1}}} % please insert
\newcommand{\del}[1]{\textcolor{red}{\sout{#1}}} % please delete
\newcommand{\chg}[2]{\textcolor{red}{\sout{#1}}{\ra}\textcolor{blue}{\uline{#2}}} % please change
\newcommand{\chk}[1]{\textcolor{ForestGreen}{#1}} % changed, please check

% comments \nb{label}{color}{text}
\newboolean{showcomments}
\setboolean{showcomments}{true}
\ifthenelse{\boolean{showcomments}}
	{\newcommand{\nb}[3]{
		{\colorbox{#2}{\bfseries\sffamily\scriptsize\textcolor{white}{#1}}}
		{\textcolor{#2}{\sf\small$\blacktriangleright$\textit{#3}$\blacktriangleleft$}}}
	 \newcommand{\version}{\emph{\scriptsize$-$Id$-$}}}
	{\newcommand{\nb}[2]{}
	 \newcommand{\version}{}}
\newcommand{\rev}[2]{\nb{Reviewer #1}{red}{#2}}
\newcommand{\ab}[1]{\nb{Alexandre}{blue}{#1}}
\newcommand{\vp}[1]{\nb{Vanessa}{orange}{#1}}

% graphics: \fig{position}{percentage-width}{filename}{caption}
\DeclareGraphicsExtensions{.png,.jpg,.pdf,.eps,.gif}
\graphicspath{{figures/}}
\newcommand{\fig}[4]{
	\begin{figure}[#1]
		\centering
		\includegraphics[width=#2\textwidth]{#3}
		\caption{\label{fig:#3}#4}
	\end{figure}}

\newcommand{\largefig}[4]{
	\begin{figure*}[#1]
		\centering
		\includegraphics[width=#2\textwidth]{#3}
		\caption{\label{fig:#3}#4}
	\end{figure*}}
	
\newcommand{\wrapfig}[5]{	
\begin{wrapfigure}{#1}{#2\textwidth}
  \begin{center}
    \includegraphics[width=#3\textwidth]{#4}
  \end{center}
  \caption{\label{fig:#4}#5}
\end{wrapfigure}}

% abbreviations
\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\eg}{\emph{e.g.,}\xspace}
\newcommand{\etc}{\emph{etc.}\xspace}
\newcommand{\etal}{\emph{et al.}\xspace}

% lists
\newenvironment{bullets}[0]
	{\begin{itemize}}
	{\end{itemize}}

\newcommand{\seclabel}[1]{\label{sec:#1}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\appref}[1]{Appendix~\ref{sec:#1}}
\newcommand{\figlabel}[1]{\label{fig:#1}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\tablabel}[1]{\label{tag:#1}}
\newcommand{\tabref}[1]{Table~\ref{fig:#1}}


%Specialized macros
\pagenumbering{arabic}

\begin{document}

\title{\Title}
%\titlerunning{\TitleShort}

\author{\Authors\\%\\[3mm]
Department of Computer Science (DCC), University of Chile
} 
%\authorrunning{\AuthorsShort}

\maketitle

%\emph{This paper makes use of colored figures. Though colors are not mandatory for full understanding, we recommend  the use of a colored printout.}

%\begin{abstract}
%An application execution profile has a meaning only when it is compared to another profile obtained from a slightly different executing context. Unfortunately, current profilers do not efficiently support performance comparison across multiple profiles. 
%As a consequence, profiling multiple executions is often realized in an ad-hoc fashion, often resulting in missing opportunities for caching.
%
%We propose multidimensional profiling as a way to repeatedly profile a software execution by varying some variables of the execution context. Having explicit execution variation points is key to precisely understand how a particular feature performance evolves along the version history of the software. We present the key ingredients to make multidimensional profiling effective and sketch the design of Rizel, an implementation in the Pharo programming language.
%\end{abstract}

%: % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\section{Profiling evolution}\seclabel{problem}

Measuring the execution performance of an application is essentially realized by varying some parameters and profiling the program execution for each variation. Identifying which method is slower, for which argument and on which object is crucial to precisely understand the reason of a slow or fast execution. Moreover, an optimal execution is often used as a target for not-so-optimal executions. Caches are inserted and optimizations are implemented until the performance of a not-so-optimal execution is close enough to the optimal one.

Unfortunately, this work is essentially realized by software engineers in an ad-hoc manner. Set of benchmarks are manually constructed to measure the application performance for each slight variation. Typical variations includes the size of the data input, version of an algorithm or a particular sequence of function executions. As surprising as it may seem, 
current profilers are either unable to compare multiple executions or offers superficial comparison facilities.

Before going into detail about the existing profilers, consider the following situation that was faced during the development of Mondrian\footnote{\url{http://moosetechnology.org/tools/mondrian}}, an agile visualization engine. Mondrian displays an arbitrary set of data as a graph, in which each node and edge has a graphical representation shaped with metrics and properties computed from the data.
%About two years ago, a user of Mondrian identified a particular visualization in which Mondrian behaved significantly slower than usual. The visualization involved an amount of nodes in a particular combination that the developers of Mondrian did not experiment before. The problem was quickly found and solved. Mondrian was about 30\% faster for the situation that was problematic. 
About two years ago, an optimization was implemented and made Mondrian 30\% faster. The optimization was carefully measured with a set of benchmarks.

During the last two years, Mondrian has been in a continuous development. As Mondrian has gained new users, new requirements have been implemented to satisfy user wishes. Whereas the range of offered features has gotten wider, the performance of Mondrian have slowly decreased for some of the benchmarks. The optimization that made Mondrian 30\% faster seems to have somehow vanished.

Tracking down the software changes that are responsible for this loose of performance is not easy, essentially because of the lack of adequate tools. Consider the commonly-used Java profilers\footnote{We have conducted all our experiments in the Pharo programming language. It is however easy to guess how it would have been perform with Java profilers.}. xprof\footnote{\url{http://bit.ly/xprofiler}} is built in the Java virtual machine and is essentially used by the Just-in-time compiler. hprof\footnote{\url{http://bit.ly/hprofiler}} is the profiler promoted by Oracle. Both xprof and hprof report the CPU time consumption for each method for an application execution. The comparison of two profiles to identify the difference of execution has to be done manually, which is a tedious and laborious task.

JProfile\footnote{\url{http://www.ej-technologies.com}} and YourKit\footnote{\url{http://www.yourkit.com}} are two popular commercial profilers. Both support a comparison of profiles by indicating the difference in absolute and relative CPU consumption time of each method. Although useful to keep track of the overall performance, knowing the difference of method execution time is insufficient to understand the reasons of the Mondrian performance variation. In addition, the call graph may significantly differ from two profiles, which seriously complicate the analysis.

\largefig{}{1.0}{final3}{Multidimensional profiling of Mondrian (6 benchmarks are run for 11 software versions).}

Understanding what happened with Mondrian, the following questions are relevant:
\begin{itemize}
\item \emph{Has the performance decreased for all the different benchmarks of Mondrian? Or only a few of them?} (Mondrian has dozen of different benchmarks to monitor each feature performance)

\item \emph{Is there a particular version of Mondrian that initiated the performance decrease?} (Mondrian has an history long of nearly 1,000 source code versions)

%\item \emph{Is there a way to relate a particular version of a method with the loose of performance?} (Each new version of Mondrian may update several methods)

%\item \emph{Is the decrease of performance equally distributed over all the objects used in a visualization, or only a part of it?} (Millions of object are used in a Mondrian visualization, it is not clear which objects contribute ``more than other'' to a slowdown)
\end{itemize}

When applied to our example with Mondrian, xprof, hprof, JProfiler and YourKit are helpless to answer any of these questions. The reason is that the profile comparison exercised by JProfiler and YourKit does not capture all the variables that these questions refer to, such as the benchmarks and software versions. Being able to profile an application along several variables is the topic of our work.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multidimensional Profiling}

We define \emph{multidimensional profiling} the activity to reason about a software execution by varying multiple variables related to its execution. Typical variables are benchmarks and software versions. Our objective is to gain a better understanding of a software execution by relating different profiles obtained from slightly different conditions. Opportunities for optimization and ways to minimize resource consumption are then easier to find.

%The rational behind multidimensional profiling is that if a software execution is particularly fast or slow for an identified situation (\ie particular values for the variables), then the situation can be exploited to improve the overall execution.

\paragraph{In a nutshell}
The ingredients to accurately exercise multidimensional profiling are:

\begin{itemize}
\item \emph{Define the variation points of the executing environment}. Variation points are defined with a set of variables $(V_1, ..., V_n)$. Each of these variables is associated to a particular aspect of the execution environment, such as software version, benchmark, parameters of a particular methods, instances of a particular class. 

\item \emph{Specify the variation of the executing environment}. Each variable may either be set to a fixed value, or may iterate over a range of values. Each iteration produces a new profile. To better measure the impact of a variable evolution, it is preferable to have all but one variable fixed. These executions result in a set of profiles $P_1, ... P_m$. 

%Automatically iterate over a set of values for a particular variable $V_i \in \{v_{i,1}, ..., v_{i,m}\}$. Each iteration produces a new profile $P_j = \textit{execute}_i(v_{i,j})$, result of executing the software with all the variables $(V_1, ..., V_n)$ into account. All the variables $V_{1..n}$ are fixed to particular values $v_{1..n}$, except $V_i$ that iterates over $\{v_{i,1}, ..., v_{i,m}\}$. These multiple executions result in a set of profiles $P_{1..m}$. 

\item \emph{Having stable profiles}. Each execution has to be repeatable and isolated from other executions. This means that two profiles $P_j$ and $P'_j$ produced by two identical executions have to be ``close enough'' to be meaningful.

\item \emph{Presenting the results.} Data must be presented for analyze to emphasize the variation of performance. The evolution of $V_i$ has to be unambiguously represented to be able to draw a conclusion about the performance evolution.
\end{itemize}

\paragraph{Implementation}
We have prototyped Rizel, a multidimensional profiler. Rizel is implemented for the Pharo Smalltalk language. The set of variables that Rizel currently consider are benchmarks and software versions. This means that for a given software, Rizel can:
\begin{itemize}
\item run a particular benchmark $b$ for each of the software versions $s_1, ... , s_k$
\item run different benchmark $b_1, ..., b_l$ for a particular software version $s$
\end{itemize}

Our profiler measures the number of messages sent by each method. It has been shown~\cite{Berg11d} that counting messages has many advantages over estimating the execution time. For example, counting messages is significantly more stable than directly measuring the time: profiling twice a same execution result in two very close profiles. Counting messages produce stable profiles.

\paragraph{Case study}
We have measured the performance of Mondrian for 6 benchmarks over 11 representative versions. 
The left-most diagram shows the evolution of the benchmarks against the versions of Mondrian. We see that each benchmark indicates a progressive degradation of the performance of Mondrian. Each of these benchmarks corresponds to a particular feature. Each feature is getting slower, not at the same pace however (\eg Benchmarks 3-6 are consuming much more time after Version 2.93. Execution time of Benchmark 1 increases after Version 2.65.)

We detail the evolution of Benchmarks 6 and 1 on the right hand side of \figref{final3}. Both histograms describes an increase of the execution time, which represents a gradual degradation of Mondrian performance.

%In our situation, isolating each feature and measuring its performance evolution is key to have a clear understanding of Mondrian performance.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion \& Future Work}

Multidimensional profiling is an innovative approach to measure software performance: crystalizing the performance of each software feature into a set of dedicated benchmarks makes it possible to precisely monitor the global performance of a software against different versions.

As a future work, in addition to the execution time we plan to extract additional metrics such as the distribution of the CPU time consumption over classes and methods. We will then affine our analysis by drilling down to the cause of a slowdown by identifying the method revision responsible of a slower performance.

We will then concentrate on identifying pattern to describe the evolution of feature performance across multiple software versions. As far as we are aware of, all these points have not been considered by the research community on software performance.

%%%%%%%%%%%%%%%%%%%%%%%%

%{\small
\bibliographystyle{plain}
\bibliography{scg}
%}

%: % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\end{document}


\ab{here}
The continue development and maintenance of sofware sometimes occasions a decline performance. The profiler is a form of dynamic program analysis that measures, for example, execution time, usage of memory, among others. The most profilers peform the analysis of a version of the software, they usualy help detect bottlenecks and some of them also provide support to remove these bottlenecks [?].

% adding new features, new requirementas and fixing bugs.

Mondrian is an open source information visualization engine, it is in constant updating adding new features, and fixing found bugs. Today the users note that Mondrian is more slow than before. And this is not the first time that the developers ask your self: ``When Mondrian became too slow?''. An approach for answer this ...

%Actually is possible to detect bottlenecks analysing in only one version, also the most common profilers can help for this.

%The behavioral evolution blueprint compares profiles of different versions of a software system and highlights performance-critical changes in the system.
These three blueprints helped us to significantly optimize Mondrian, an open source visualization engine.

We propose keep trace of perfomances to the differents versions of the software to know in what version decline the performance. This help us to know what feature or bug fixed ocacione? the slow performance, in other words, we could found the cause of the decline performance.
%For this approach is nescesary a well define execution escenario, but found the most slow method is not enough because â€¦ 

\section{Challenges in Keeping trace of Performances}\seclabel{problem}

Metric to measure the performance.- A usual way to measure is get the execution time involved in the execution, but this metric is volatile, because execute the same instructions in the same version can throws diferent execution time. Because it depends of differents factores, cpu state, history. We propose count Messages as a Proxy for Average Execution Time [?].

Defining a benchmark.- This could be the most difficult task, because it will be executed in differents versions of our code and it should be good enough to reproduce the performance deficience.

Another challenge is handle a several number of versions,

%An application could  worsen or improve in differents version, 



%The most difficult task is define an execution scenario, that could be executed in the versions of the software that we are analysing without throws an exception and good enough to reproduce the performance deficience.

%Execution time is not deterministic, in these sence if we execute several times the same code, a profile show us diferent times. 
%This is a problem in our case beacause we wan compare the performance of many versions of the applications. 
%Use the execution time to compare the performance is not the better way. We propose use the number of messages to compare the performance, the number of messages is so useful to detect bottlenecks as the execution time [?].


%: % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\section{Rizel: Performances across program versions}\seclabel{solution}

Rizel is a tool that examine the performances across program versions, providing a friendly visualization. 

that allows run a benchmark in a set of user defined versions keeping track of the profiled information in each execution.
%Rizel is an application based in Spy[?] that allow evaluate a well defined execution scenario across program versions.

%Rizel provide a friendly visualizations that show the performance evolution \figref{}.
%In a global way or by method.

%\ab{What are the ingredients of Rizel? }
\begin{itemize}
\item Running a benchmark for a particular application versions.
\item Counting messages as a Proxy for Average Execution Time.
\item Jumping between versions using Dicotomic search to find most recient the decline performance.
\item Minimizing the effect (\eg caches, JIT) between multiple run.
\item Exposing the evolution of the performances.
\end{itemize}	

It uses compteour profiler [?] to retrieve the number of messages involved in an execution. Rizel search the most recient decline peformance using dicotomic search, for this first sort the versions cronologically and tries to execute the benchmark as little as possible. 

\fig{}{0.5}{picture1}{}
\fig{}{0.5}{picture2}{}

%\ab{Screenshot of Rizel}

%: % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\section{Conclusion \& Future Work}\seclabel{conclusion}

Keep track of profiling information, for example, number of messages, execution time, the usage of memory, among others. Can help us to aid program optimization.

In our aproach we propose detect the cause of decline performance. Compare the peformance across program versions is only our firts aproach in differentation profiling.

%\ab{What is your vision?}

%\paragraph{Acknowledments} We thanks 

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

