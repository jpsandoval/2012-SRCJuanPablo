% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%\documentclass[runningheads]{llncs}
%\documentclass[10pt,letterpaper,twocolumn]{article}
\documentclass{sig-alternate}


% packages
\usepackage{xspace}
\usepackage{ifthen}
\usepackage{amsbsy}
\usepackage{amssymb}
\usepackage{balance}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{needspace}
\usepackage{microtype}
\usepackage{bold-extra}
\usepackage{subfigure}
\usepackage{wrapfig}


% constants
%\newcommand{\Title}{Tracking Performance Evolution}
\newcommand{\Title}{Multidimensional Profiling}
\newcommand{\TitleShort}{\Title}
\newcommand{\Authors}{Juan Pablo Sandoval}
\newcommand{\AuthorsShort}{J. P. Sandoval}

% references
\usepackage[colorlinks]{hyperref}
\usepackage[all]{hypcap}
\setcounter{tocdepth}{2}
\hypersetup{
	colorlinks=true,
	urlcolor=black,
	linkcolor=black,
	citecolor=black,
	plainpages=false,
	bookmarksopen=true,
	pdfauthor={\Authors},
	pdftitle={\Title}}

\def\chapterautorefname{Chapter}
\def\appendixautorefname{Appendix}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\figureautorefname{Figure}
\def\tableautorefname{Table}
\def\listingautorefname{Listing}

% source code
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{listings}
\definecolor{source}{gray}{0.9}
\lstset{
	language={},
	% characters
	tabsize=3,
	upquote=true,
	escapechar={!},
	keepspaces=true,
	breaklines=true,
	alsoletter={\#:},
	breakautoindent=true,
	columns=fullflexible,
	showstringspaces=false,
	basicstyle=\footnotesize\sffamily,
	% background
	frame=single,
    framerule=0pt,
	backgroundcolor=\color{source},
	% numbering
	numbersep=5pt,
	numberstyle=\tiny,
	numberfirstline=true,
	% captioning
	captionpos=b,
	% formatting (html)
	moredelim=[is][\textbf]{<b>}{</b>},
	moredelim=[is][\textit]{<i>}{</i>},
	moredelim=[is][\color{red}\uwave]{<u>}{</u>},
	moredelim=[is][\color{red}\sout]{<del>}{</del>},
	moredelim=[is][\color{blue}\underline]{<ins>}{</ins>}}
\newcommand{\ct}{\lstinline[backgroundcolor=\color{white},basicstyle=\footnotesize\ttfamily]}
\newcommand{\lct}[1]{{\small\tt #1}}

% tikz
% \usepackage{tikz}
% \usetikzlibrary{matrix}
% \usetikzlibrary{arrows}
% \usetikzlibrary{external}
% \usetikzlibrary{positioning}
% \usetikzlibrary{shapes.multipart}
% 
% \tikzset{
% 	every 	picture/.style={semithick},
% 	every text node part/.style={align=center}}

% proof-reading
\usepackage{xcolor}
\usepackage[normalem]{ulem}
\newcommand{\ra}{$\rightarrow$}
\newcommand{\ugh}[1]{\textcolor{red}{\uwave{#1}}} % please rephrase
\newcommand{\ins}[1]{\textcolor{blue}{\uline{#1}}} % please insert
\newcommand{\del}[1]{\textcolor{red}{\sout{#1}}} % please delete
\newcommand{\chg}[2]{\textcolor{red}{\sout{#1}}{\ra}\textcolor{blue}{\uline{#2}}} % please change
\newcommand{\chk}[1]{\textcolor{ForestGreen}{#1}} % changed, please check

% comments \nb{label}{color}{text}
\newboolean{showcomments}
\setboolean{showcomments}{true}
\ifthenelse{\boolean{showcomments}}
	{\newcommand{\nb}[3]{
		{\colorbox{#2}{\bfseries\sffamily\scriptsize\textcolor{white}{#1}}}
		{\textcolor{#2}{\sf\small$\blacktriangleright$\textit{#3}$\blacktriangleleft$}}}
	 \newcommand{\version}{\emph{\scriptsize$-$Id$-$}}}
	{\newcommand{\nb}[2]{}
	 \newcommand{\version}{}}
\newcommand{\rev}[2]{\nb{Reviewer #1}{red}{#2}}
\newcommand{\ab}[1]{\nb{Alexandre}{blue}{#1}}
\newcommand{\vp}[1]{\nb{Vanessa}{orange}{#1}}

% graphics: \fig{position}{percentage-width}{filename}{caption}
\DeclareGraphicsExtensions{.png,.jpg,.pdf,.eps,.gif}
\graphicspath{{figures/}}
\newcommand{\fig}[4]{
	\begin{figure}[#1]
		\centering
		\includegraphics[width=#2\textwidth]{#3}
		\caption{\label{fig:#3}#4}
	\end{figure}}

\newcommand{\largefig}[4]{
	\begin{figure*}[#1]
		\centering
		\includegraphics[width=#2\textwidth]{#3}
		\caption{\label{fig:#3}#4}
	\end{figure*}}
	
\newcommand{\wrapfig}[5]{	
\begin{wrapfigure}{#1}{#2\textwidth}
  \begin{center}
    \includegraphics[width=#3\textwidth]{#4}
  \end{center}
  \caption{\label{fig:#4}#5}
\end{wrapfigure}}

% abbreviations
\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\eg}{\emph{e.g.,}\xspace}
\newcommand{\etc}{\emph{etc.}\xspace}
\newcommand{\etal}{\emph{et al.}\xspace}

% lists
\newenvironment{bullets}[0]
	{\begin{itemize}}
	{\end{itemize}}

\newcommand{\seclabel}[1]{\label{sec:#1}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\appref}[1]{Appendix~\ref{sec:#1}}
\newcommand{\figlabel}[1]{\label{fig:#1}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\tablabel}[1]{\label{tag:#1}}
\newcommand{\tabref}[1]{Table~\ref{fig:#1}}


%Specialized macros
\pagenumbering{arabic}

\begin{document}

\title{\Title}
%\titlerunning{\TitleShort}

\author{\Authors\\[3mm]
Department of Computer Science (DCC)\\ University of Chile, Santiago, Chile\\[1 ex]
} 
%\authorrunning{\AuthorsShort}

\maketitle

%\emph{This paper makes use of colored figures. Though colors are not mandatory for full understanding, we recommend  the use of a colored printout.}

\begin{abstract}
%What is the problem
An application execution profile has a meaning only when it is compared to another profile obtained from a slightly different condition. Knowing that a method is faster for a particular input or software version is essential ...
Current profiler do not support performance comparison across multiple profiles. 
%Why the problem is a problem
As a consequence, profiling multiple executions is often realized in an ad-hoc fashion, often resulting in missing opportunities for caching.

%What is the surprise
Providing multiple execution 

%What are the consequences
We 


%The common profilers use the runtime information to aid program optimization in a particular version of software.
%Over time, programs as applications continue to develop and repeatedly updating it for various reasons and producing differents versions. Sometimes this proccess can decline the performance. 
%
%I this paper we propose use the profiling information to compare the different versions of the software to found the cause that decline the performance. Also present to Rizel an approach to track the peformance evolution, that allows display a easy peformance differentiation visualization.

\end{abstract}

%: % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\section{Introduction}\seclabel{problem}

Measuring the execution performance of an application is essentially realized by varying some parameters and profiling the program execution for each variation. Identifying which method is slower, for which argument and on which objects is crucial to precisely understand the reason of a slow or fast execution. Moreover, an optimal execution is often used as a target for not-so-optimal executions. Caches are inserted and optimizations are implemented until the performance of a not-so-optimal execution is close enough to the optimal one.

Unfortunately, this work is essentially realized by the software engineer in an ad-hoc manner. Set of benchmarks are manually constructed to measure the application performance for each slight variation. Typical variations includes the size of the data input, versions of an algorithm or a sequence of function executions. As surprising as it may seem, 
current profilers are either unable to compare multiple executions, or offers primitive comparison facilities.

Before going into detail about the existing profilers, consider the following situation that was faced during the development of Mondrian\footnote{\url{http://moosetechnology.org/tools/mondrian}}, an agile visualization engine. Mondrian represents any arbitrary data as a graph, in which nodes and edges have graphical representation that depends on metrics and properties computed from the data.
About two years ago, one user of Mondrian identified a particular visualization in which Mondrian behaved significantly slower than usual. The visualization involved an amount of nodes in a particular combination that the developers of Mondrian did not experiment before. The problem was quickly found and solved. Mondrian was about 30\% faster for the situation that was problematic. 

During the last two years, Mondrian has been in a continuous development. As Mondrian has gained new users, new requirements have been implemented to satisfy user wishes. Whereas the range of offered features is getting wider, the performance of Mondrian have slowly decreased for some of the benchmarks. The optimization that helps reduce the execution time by 30\% seems to have vanished somehow for some particular situations. 

Tracking down the major software changes that are responsible for this loose of performance is not easy, essentially for the lack of adequate tools. Consider the commonly-used Java profilers. xprof\footnote{\url{http://bit.ly/xprofiler}} is built-in the Java virtual machine and is essentially used by the Just-in-time compiler. hprof\footnote{\url{http://bit.ly/hprofiler}} is the profiler promoted by Oracle. Both xprof and hprof report the CPU time consumption for each methods for an application execution. The comparison of two profiles has to be done manually, which is a tedious and laborious task.

JProfile\footnote{\url{http://www.ej-technologies.com}} and YourKit\footnote{\url{http://www.yourkit.com}} are two popular commercial profilers. Both support a comparison of profiles by indicating the difference in absolute and relative CPU consumption time of each method. Although useful to keep track of the overall performance, knowing the difference of method execution is insufficient to understand the reasons of a performance variation. 

For our example with Mondrian, we have the following questions:
\begin{itemize}
\item \emph{Has the performance decreased for all the different benchmarks of Mondrian? Or only a few of them?} (Mondrian has dozen of different benchmarks to monitor the performance of some particular features)
\item \emph{Is there a particular version of Mondrian that initiated the performance decrease?} (Mondrian has an history long of nearly 1,000 source code versions)
\item \emph{Is there a way to relate a particular version of a method with the loose of performance?} 
\item \emph{Is the decrease of performance equally distributed over all the objects used in a visualization, or only a part of it?} (Millions of object are used in a Mondrian visualization, it is not clear which objects contribute ``more than other'' to a slowdown)
\end{itemize}

When applied to our example with Mondrian, JProfiler and YourKit are helpless to answer any of those questions. The reason is that the comparison exercised by JProfiler and YourKit does not capture all the variables that these questions refer to, such as the benchmarks, software versions, method versions, and objects. Being able to profile an application along several variables is the topic of our work.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multidimensional Profiling}

We define \emph{multidimensional profiling} the activity to reason about a software execution based on varying more than one variables. Our objective is to obtain a better understanding of a software execution by relating different execution profiles. This understanding is then be used to find opportunities for optimization.

The ingredients to accurately monitor the performance evolution are:

\begin{itemize}
\item Have a set of variables of interest $(V_1, ..., V_n)$ to describe the executing environment. Each of these variables describe a particular aspect of the execution environment, such as software version, a benchmark, parameters of a particular methods, instances of a particular class. 

\item Automatically iterate over a set of value for a particular variable $V_i \in \{v_{i,1}, ..., v_{i,m}\}$. Each iteration produces a new profile $P_j = \textit{execute}_i(v_{i,j})$, result of executing the software with all the variables $(V_1, ..., V_n)$ into account. All the variables $V_{1..n}$ are fixed to a particular value $v_i$, except $V_i$ that iterates over its values. These multiple executions result in a set of profiles $P_{1...m}$. 

\item Each execution has to be repeatable and isolated from other executions. This means that two profiles $P_j$ and $P'_j$ resulting from two executions with the same value $v_{i,j}$ have to be ``close enough'' to be meaningful.

\item Give a representation of the profile difference. The evolution of $V_i$ has to be unambiguously represented to be able to draw a conclusion about the performance evolution.
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%


%{\small
\bibliographystyle{plain}
\bibliography{scg}
%}

%: % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\end{document}


\ab{here}
The continue development and maintenance of sofware sometimes occasions a decline performance. The profiler is a form of dynamic program analysis that measures, for example, execution time, usage of memory, among others. The most profilers peform the analysis of a version of the software, they usualy help detect bottlenecks and some of them also provide support to remove these bottlenecks [?].

% adding new features, new requirementas and fixing bugs.

Mondrian is an open source information visualization engine, it is in constant updating adding new features, and fixing found bugs. Today the users note that Mondrian is more slow than before. And this is not the first time that the developers ask your self: ``When Mondrian became too slow?''. An approach for answer this


We propose the Behavioral evolution blueprint as an effective and intuitive visualization
that complements Structural distribution blueprint and Behavioral distribution blueprint, two
visualizations of a unique profile snapshot. This third blueprint focuses on differencing profiles
of two or more snapshots. A visualization always differentiates two profiles of the same application
and benchmark, but for different software versions. Behavioral evolution blueprint helps tracking
down the cause of a performance increase or decrease


%Actually is possible to detect bottlenecks analysing in only one version, also the most common profilers can help for this.

%The behavioral evolution blueprint compares profiles of different versions of a software system and highlights performance-critical changes in the system.
These three blueprints helped us to significantly optimize Mondrian, an open source visualization engine.
%For this approach is nescesary a well define execution escenario, but found the most slow method is not enough because … 

\section{Challenges in Keeping trace of Performances}\seclabel{problem}

Metric to measure the performance.- A usual way to measure is get the execution time involved in the execution, but this metric is volatile, because execute the same instructions in the same version can throws diferent execution time. Because it depends of differents factores, cpu state, history. We propose count Messages as a Proxy for Average Execution Time [?].

Defining a benchmark.- This could be the most difficult task, because it will be executed in differents versions of our code and it should be good enough to reproduce the performance deficience.

We propose keep trace of perfomances to the differents versions of the software to know in what version decline the performance. This help us to know what feature or bug fixed ocacione? the slow performance, in other words, we could found the cause of the decline performance.

%An application could  worsen or improve in differents version, 



%The most difficult task is define an execution scenario, that could be executed in the versions of the software that we are analysing without throws an exception and good enough to reproduce the performance deficience.

%Execution time is not deterministic, in these sence if we execute several times the same code, a profile show us diferent times. 
%This is a problem in our case beacause we wan compare the performance of many versions of the applications. 
%Use the execution time to compare the performance is not the better way. We propose use the number of messages to compare the performance, the number of messages is so useful to detect bottlenecks as the execution time [?].


%: % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\section{Rizel: Performances across program versions}\seclabel{solution}

Rizel is a tool that examine the performances across program versions, providing a friendly visualization. 

that allows run a benchmark in a set of user defined versions keeping track of the profiled information in each execution.
%Rizel is an application based in Spy[?] that allow evaluate a well defined execution scenario across program versions.

%Rizel provide a friendly visualizations that show the performance evolution \figref{}.
%In a global way or by method.

%\ab{What are the ingredients of Rizel? }
\begin{itemize}
\item Running a benchmark for a particular application versions.
\item Counting messages as a Proxy for Average Execution Time.
\item Jumping between versions using Dicotomic search to find most recient the decline performance.
\item Minimizing the effect (\eg caches, JIT) between multiple run.
\item Exposing the evolution of the performances.
\end{itemize}	

It uses compteour profiler [?] to retrieve the number of messages involved in an execution. Rizel search the most recient decline peformance using dicotomic search, for this first sort the versions cronologically and tries to execute the benchmark as little as possible. 

\fig{}{0.5}{picture1}{}
\fig{}{0.5}{picture2}{}

%\ab{Screenshot of Rizel}

%: % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\section{Conclusion \& Future Work}\seclabel{conclusion}

Keep track of profiling information, for example,number of messages, execution time, the usage of memory, among others. Can help us to aid program optimization.

In our aproach we propose detect the cause of decline performance. Compare the peformance across program versions is only our firts aproach in differentation profiling.

%\ab{What is your vision?}

%\paragraph{Acknowledments} We thanks 

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

